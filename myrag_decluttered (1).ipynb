{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lVzUwlTAObxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddf26e0-cc1a-427a-e78c-50837715924b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Setup Environment and Install Packages\n",
        "import os\n",
        "os.makedirs('rag_chatbot', exist_ok=True)\n",
        "os.makedirs('rag_chatbot/docs', exist_ok=True)\n",
        "os.chdir('/content/rag_chatbot')\n",
        "\n",
        "!pip install -q google-generativeai langchain langchain-google-genai chromadb PyMuPDF langchain-community sentence-transformers\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure API and Imports\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Get API key and configure\n",
        "API_KEY = userdata.get('MY_GEMINI_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "genai.configure(api_key=API_KEY)\n",
        "print(\"âœ… API configured!\")"
      ],
      "metadata": {
        "id": "0k8pvaKpOgFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da4ae84-94d5-46e9-88cb-0b302c78b740"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload Documents\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.pdf'):\n",
        "        shutil.move(filename, f'docs/{filename}')\n",
        "        print(f\"âœ… {filename}\")\n",
        "\n",
        "print(f\"ğŸ“š {len([f for f in os.listdir('docs') if f.endswith('.pdf')])} PDFs ready\")"
      ],
      "metadata": {
        "id": "B7Gru4rlQS_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f1540736-2987-4568-8582-ec4ae1f82a4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f82ad970-c8c3-4e4d-ab54-1542e64cec43\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f82ad970-c8c3-4e4d-ab54-1542e64cec43\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NIPS-2017-attention-is-all-you-need-Paper.pdf to NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
            "âœ… NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
            "ğŸ“š 2 PDFs ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and Process Documents\n",
        "documents = []\n",
        "for filename in os.listdir(\"docs\"):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        loader = PyMuPDFLoader(f\"docs/{filename}\")\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"ğŸ“„ {len(documents)} pages â†’ {len(chunks)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VMky6pNQdzL",
        "outputId": "75eb5165-e843-4c16-d566-21faf3695e76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ 23 pages â†’ 76 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create Vector Database\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectordb = Chroma.from_documents(chunks, embeddings, persist_directory='db')\n",
        "retriever = vectordb.as_retriever()\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
        "print(\"ğŸ—„ï¸ Vector database ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyan5eiaQfmg",
        "outputId": "a2d69412-8073-4c54-b6a8-fbda13399c1d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—„ï¸ Vector database ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Chat Functions\n",
        "def ask_llm_only(query):\n",
        "    prompt = f\"Answer only if confident. If unsure, say 'UNSURE'.\\n\\nQuestion: {query}\"\n",
        "    response = llm.invoke(prompt)\n",
        "    content = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "    if \"unsure\" in content.lower() or \"don't know\" in content.lower():\n",
        "        return None\n",
        "    return content\n",
        "\n",
        "def ask_with_docs(query):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "    return response[\"result\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "AmwoVmthQkPK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Chatbot\n",
        "def chat():\n",
        "    print(\"\\nğŸ’¬ RAG Chatbot Started! (type 'exit' to stop)\")\n",
        "    print(\"ğŸ” Watch for [DIRECT] vs [RAG] indicators!\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \").strip()\n",
        "\n",
        "        if query.lower() == 'exit':\n",
        "            print(\"ğŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Try LLM first\n",
        "        response = ask_llm_only(query)\n",
        "\n",
        "        if response:\n",
        "            print(f\"\\n[DIRECT] ğŸ§  AI: {response}\\n\")\n",
        "        else:\n",
        "            print(\"ğŸ” LLM unsure â†’ Searching documents...\")\n",
        "            response = ask_with_docs(query)\n",
        "            print(f\"\\n[RAG] ğŸ“š AI: {response}\\n\")\n",
        "\n",
        "# Start the enhanced chatbot\n",
        "chat()"
      ],
      "metadata": {
        "id": "JR8HabVCRIx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350f2fba-6b64-41b5-b887-62385cfbb6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¬ RAG Chatbot Started! (type 'exit' to stop)\n",
            "ğŸ” Watch for [DIRECT] vs [RAG] indicators!\n",
            "\n",
            "You: \"What is attention mechanism in general?\n",
            "\n",
            "[DIRECT] ğŸ§  AI: Attention mechanisms are a family of techniques that allow a neural network to focus on different parts of its input when processing it.  Instead of processing the entire input equally, attention assigns weights to different parts, emphasizing the most relevant information for the current task.  This allows the network to effectively handle long sequences or complex inputs by selectively focusing on the most important elements.\n",
            "\n",
            "You: What are transformers in machine learning?\n",
            "\n",
            "[DIRECT] ğŸ§  AI: Transformers are a type of neural network architecture primarily known for their effectiveness in processing sequential data, such as text and time series.  They utilize a mechanism called \"self-attention\" to weigh the importance of different parts of the input sequence when generating an output.  This allows them to capture long-range dependencies within the data more effectively than previous architectures like recurrent neural networks.\n",
            "\n",
            "You: What BLEU score did the Transformer achieve on WMT 2014 English-to-German?\n",
            "ğŸ” LLM unsure â†’ Searching documents...\n",
            "\n",
            "[RAG] ğŸ“š AI: The Transformer (big) model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.  The base model achieved a BLEU score of 27.3.\n",
            "\n",
            "You: What are the specific authors of the Attention is All You Need paper?\n",
            "\n",
            "[DIRECT] ğŸ§  AI: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
            "\n",
            "You: How many warmup steps did they use for the learning rate schedule?\n",
            "ğŸ” LLM unsure â†’ Searching documents...\n",
            "\n",
            "[RAG] ğŸ“š AI: They used 4000 warmup steps.\n",
            "\n",
            "You: What is the exact formula for the learning rate schedule in equation 3?\n",
            "ğŸ” LLM unsure â†’ Searching documents...\n",
            "\n",
            "[RAG] ğŸ“š AI: The formula for the learning rate schedule is:\n",
            "\n",
            "`lrate = dâ»â°Â·âµmodel * min(step_numâ»â°Â·âµ, step_num * warmup_stepsâ»Â¹.âµ)`\n",
            "\n",
            "You: What is a transformer model?\n",
            "\n",
            "[DIRECT] ğŸ§  AI: A transformer model is a neural network architecture that relies on a mechanism called self-attention to process sequential input data, such as text or time series.  Unlike recurrent neural networks (RNNs), transformers process the entire input sequence simultaneously, allowing for parallelization and faster training.  They are known for their ability to capture long-range dependencies in data and have achieved state-of-the-art results in various natural language processing tasks.\n",
            "\n",
            "You: What were the exact beta values used for the Adam optimizer in the Attention is All You Need paper?\n",
            "\n",
            "[DIRECT] ğŸ§  AI: Î²â‚ = 0.9, Î²â‚‚ = 0.999\n",
            "\n",
            "You: What were the vocabulary sizes for English-German vs English-French datasets?\n",
            "ğŸ” LLM unsure â†’ Searching documents...\n",
            "\n",
            "[RAG] ğŸ“š AI: For English-German, the vocabulary size was about 37,000 tokens.  For English-French, it was 32,000 word-pieces.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Gemini\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "print(\"API key configured successfully!\")\n",
        "\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = MY_GEMINI_KEY\n",
        "genai.configure(api_key=MY_GEMINI_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSYL4cyWRXKx",
        "outputId": "32acbe47-b44b-4dbb-9c14-a2b90fef2f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what's in your documents directory\n",
        "!ls -la documents/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GIzytiGRpHj",
        "outputId": "bc6b09f2-8275-4d76-e592-07edfa90f14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2164\n",
            "drwxr-xr-x 2 root root    4096 Aug 20 02:31 .\n",
            "drwxr-xr-x 4 root root    4096 Aug 20 02:31 ..\n",
            "-rw-r--r-- 1 root root 2206753 Aug 20 02:31 3-17_NERVOUS_HANDOUT.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"Upload your PDF files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to the documents directory\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.pdf'):\n",
        "        shutil.move(filename, f'documents/{filename}')\n",
        "        print(f\"âœ… Moved {filename} to documents/\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ {filename} is not a PDF file, skipping...\")\n",
        "\n",
        "# Check what's now in documents\n",
        "print(\"\\nDocuments directory now contains:\")\n",
        "!ls -la documents/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "NVki8LWhRsDr",
        "outputId": "ce703a96-04de-46f8-d227-d8a52d7d5d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your PDF files:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-46cf8402-a1cd-4918-b468-2d8e71544681\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-46cf8402-a1cd-4918-b468-2d8e71544681\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 3-17_NERVOUS_HANDOUT.pdf to 3-17_NERVOUS_HANDOUT.pdf\n",
            "âœ… Moved 3-17_NERVOUS_HANDOUT.pdf to documents/\n",
            "\n",
            "Documents directory now contains:\n",
            "total 2164\n",
            "drwxr-xr-x 2 root root    4096 Aug 20 02:42 .\n",
            "drwxr-xr-x 4 root root    4096 Aug 20 02:42 ..\n",
            "-rw-r--r-- 1 root root 2206753 Aug 20 02:42 3-17_NERVOUS_HANDOUT.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import AsyncChromiumLoader #directlyt import the data from an URL\n",
        "from langchain.document_transformers import Html2TextTransformer #converts html to text data\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "6oEECyc7RvRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = []\n",
        "for filename in os.listdir(\"documents\"):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        loader = PyMuPDFLoader(os.path.join(\"documents\", filename))\n",
        "        document.extend(loader.load())\n",
        "\n",
        "print(f\"Total pages loaded: {len(document)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOS0k3hlR3R0",
        "outputId": "568ff354-1c26-4bbf-fb2e-36addc43d669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages loaded: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "text_chunks = text_splitter.split_documents(document)\n",
        "\n",
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZaFFdkdR54e",
        "outputId": "026a6c7d-7acf-428e-a899-693b52d5387e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use in embeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embedding = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    google_api_key=MY_GEMINI_KEY  # Use your variable name here\n",
        ")"
      ],
      "metadata": {
        "id": "zoMRgoVCWFmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector database\n",
        "persist_directory = 'db'\n",
        "vectordb = Chroma.from_documents(documents=text_chunks,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "ASOZl1INR-2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()\n"
      ],
      "metadata": {
        "id": "r-A2AxiESAQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n"
      ],
      "metadata": {
        "id": "yjEnTYL0UrYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Function to Ask LLM First\n",
        "def ask_llm_first(query):\n",
        "    \"\"\"Ask the LLM first. If it confidently answers, return it. Otherwise, proceed to VectorDB.\"\"\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an AI assistant. Answer only if you are confident about your response. \"\n",
        "        \"If you do not know the answer, simply respond with: 'I don't know.' \"\n",
        "        \"Do not guess or make assumptions.\\n\\n\"\n",
        "        f\"User Question: {query}\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(system_prompt)  # Query the LLM\n",
        "\n",
        "    # Extract text content from AIMessage object\n",
        "    if hasattr(response, \"content\"):\n",
        "        response_text = response.content  # Correct way to get the response text\n",
        "    else:\n",
        "        response_text = str(response)  # Fallback in case 'content' is missing\n",
        "\n",
        "    # Define phrases that indicate uncertainty\n",
        "    vague_responses = [\n",
        "        \"i don't know\", \"i am not sure\", \"i need more information\", \"please provide more details\",\n",
        "        \"i cannot determine\", \"i am unable to\", \"i do not have enough context\",\n",
        "        \"i need more context\", \"i do not have enough information\", \"i require additional details\"\n",
        "    ]\n",
        "\n",
        "     # Debugging: Print each phrase check\n",
        "    for phrase in vague_responses:\n",
        "        if phrase in response_text.lower():\n",
        "            print(f\"âœ… Match found: '{phrase}' is in LLM response!\")\n",
        "            print(\"âš ï¸ LLM is unsure. Fetching data from VectorDB...\")\n",
        "            return False  # Indicate retrieval is needed\n",
        "\n",
        "    print(\"ğŸ¤– LLM confidently answered without RAG.\")\n",
        "    return response_text  # Return LLM's answer"
      ],
      "metadata": {
        "id": "h53WKe8GUtDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Function to Perform RAG Retrieval\n",
        "def ask_with_rag(query):\n",
        "    \"\"\"Retrieve documents from VectorDB and generate an answer using LLM with context.\"\"\"\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "    print(\"\\nğŸ¤– AI Agent's Response (with RAG):\")\n",
        "    return response[\"result\"]\n"
      ],
      "metadata": {
        "id": "dRJgF97fUvga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_conversation():\n",
        "    \"\"\"Start an interactive chat session with the LLM and VectorDB.\"\"\"\n",
        "    print(\"\\nğŸ’¬ Start chatting with the AI! (Type 'exit' to stop)\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input dynamically\n",
        "        user_query = input(\"ğŸ‘¤ You: \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"ğŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Step 1: Ask LLM first\n",
        "        llm_response = ask_llm_first(user_query)\n",
        "\n",
        "        # Step 2: If LLM is unsure (False), use VectorDB retrieval\n",
        "        if llm_response is False:\n",
        "            final_answer = ask_with_rag(user_query)\n",
        "            print(f\"\\nğŸ¤– AI: {final_answer}\\n\")\n",
        "        else:\n",
        "            print(f\"\\nğŸ¤– AI: {llm_response}\\n\")"
      ],
      "metadata": {
        "id": "S49MoWPSUxOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_conversation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E8SN8QLaUzHs",
        "outputId": "901489ac-0c53-4d78-d194-3845da0fd74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¬ Start chatting with the AI! (Type 'exit' to stop)\n",
            "\n",
            "ğŸ‘¤ You: what drugs affect the nervous system\n",
            "ğŸ¤– LLM confidently answered without RAG.\n",
            "\n",
            "ğŸ¤– AI: Many drugs affect the nervous system.  This is a very broad category.  To give a useful answer, I need more specifics.  What kind of effect are you interested in? (e.g., stimulants, depressants, analgesics, etc.)\n",
            "\n",
            "ğŸ‘¤ You: stimulants please\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: Based on the provided text, caffeine and nicotine are stimulants.  Caffeine stimulates the central nervous system by suppressing melatonin and promoting adrenalin. Nicotine also stimulates the central nervous system, causing an increased release of neurotransmitters associated with pleasure.\n",
            "\n",
            "ğŸ‘¤ You: what about anticonvulsants\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: I'm sorry, but I don't have any information about anticonvulsants in this context.\n",
            "\n",
            "ğŸ‘¤ You: what about heroin\n",
            "ğŸ¤– LLM confidently answered without RAG.\n",
            "\n",
            "ğŸ¤– AI: Heroin is a highly addictive opioid drug derived from morphine.  It's illegal in most countries and carries significant risks of overdose and serious health consequences, including respiratory depression, collapsed veins, and infections.  Long-term use can lead to severe health problems and addiction.\n",
            "\n",
            "ğŸ‘¤ You: what about marijuana\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: THC, the main active ingredient in marijuana, binds to protein receptors on nerve cells in the central nervous system.  This initiates a chemical reaction that produces various effects, including suppression of the hippocampus (memory and learning centers) in the brain.\n",
            "\n",
            "ğŸ‘¤ You: what are the regions of cerebrum'\n",
            "ğŸ¤– LLM confidently answered without RAG.\n",
            "\n",
            "ğŸ¤– AI: The cerebrum is divided into four lobes: frontal, parietal, temporal, and occipital.\n",
            "\n",
            "ğŸ‘¤ You: what are the three special regions\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: I'm sorry, but based on the provided text, I cannot answer your question.  The text mentions the brain's processing of sound and smell, but it does not identify three specific \"special regions\" of the brain.\n",
            "\n",
            "ğŸ‘¤ You: in lobes of cerebrum, what are the three special regions\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: The three special regions in the cerebrum mentioned in the provided text are Broca's area, Wernicke's area, and the Limbic System.\n",
            "\n",
            "ğŸ‘¤ You: explain them\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: The provided text explains how the brain processes information from the ears to interpret sounds and motion/acceleration, and how the sense of smell is linked to emotions, memory, and taste.  There is no further explanation available in the provided text.\n",
            "\n",
            "ğŸ‘¤ You: explain those three regions\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: I'm sorry, but I don't know which three regions you are referring to.  The provided text mentions the brain's processing of sound and smell, but doesn't define or name specific brain regions.\n",
            "\n",
            "ğŸ‘¤ You: in lobes of cerebrum, what are the three special regions, explain them\n",
            "âœ… Match found: 'i don't know' is in LLM response!\n",
            "âš ï¸ LLM is unsure. Fetching data from VectorDB...\n",
            "\n",
            "ğŸ¤– AI Agent's Response (with RAG):\n",
            "\n",
            "ğŸ¤– AI: The provided text mentions three special regions within the cerebrum:\n",
            "\n",
            "*   **Broca's area:** Located in the frontal lobe, it's crucial for speech production.\n",
            "*   **Wernicke's area:**  This area is involved in language comprehension and the production of meaningful speech.\n",
            "*   **Limbic system:**  A group of brain structures (including the amygdala, hippocampus, septum, and basal ganglia) that regulate the expression of emotions and emotional memory.\n",
            "\n",
            "ğŸ‘¤ You: thank you\n",
            "ğŸ¤– LLM confidently answered without RAG.\n",
            "\n",
            "ğŸ¤– AI: You're welcome.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3835696535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1792809752.py\u001b[0m in \u001b[0;36mstart_conversation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Get user input dynamically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ‘¤ You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Exit condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSk-LMzGU1Cw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}